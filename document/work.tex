\documentclass[UTF8]{article}

\usepackage[margin=1in]{geometry}
\usepackage{enumerate}
\usepackage[colorlinks,linkcolor=black]{hyperref}
\usepackage{ctex}
\usepackage{titling}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{array}
\usepackage{float}
\usepackage{subfigure}





\title{人工智能大作业报告}
\author{田佳析}
% todo: 修改标题格式

\begin{document}

\maketitle
\tableofcontents

\section{监督学习}
\subsection{问题描述}
能够用拍照方式识别五子棋下棋过程中当前落子的位置，
识别程序中应使用到监督学习算法。
\subsection{方法描述}
使用yolov5实现对五子棋的目标识别，使用编写的五子棋界面生成训练
和测试数据。下载预训练模型后，进行迁移学习，训练得到识别黑白棋的
模型。

\subsection{步骤描述}

\begin{enumerate}
    \item 利用五子棋界面生成训练数据
    
    \subitem 对于五子棋界面的实现，考虑到界面不是本次作业实现的
    主要目标，所以使用tkinter库来实现五子棋简易界面。

    \subitem yolov5所用的标签格式为\ [物体类型\ 物体中心
    x值\ 物体中心y值\ 物体长度\ 物体宽度]。随机生成五子棋
    局面，绘制界面，生成标签，训练数据大小为200张图片

    \subitem 生成训练数据的代码位于Python/window/create\_train\_data.py
    文件中。
    
    \item 下载yolov5项目，下载预训练模型
    
    \subitem yolov5所用的模型代码来自github上的一个
    \href{https://github.com/ultralytics/yolov5}{项目}。
    考虑到黑白棋的特征明显，识别出来的难度不大，
    所以我使用的预训练模型为YOLOv5s模型。在进行100次的训练后
    能够较好地得到结果。

    \item 对模型进行训练
    
    \subitem 模型训练过程中准确率和召回率和IoU阈值为0.5时的mAP值变化如下：

    \begin{figure}[H]
        \centering
        \subfigure[准确率]{
        \includegraphics[width=0.25\textwidth]{img/precision.PNG}
        }
        \subfigure[召回率]{
        \includegraphics[width=0.27\textwidth]{img/recall.PNG}
        }
        \subfigure[mAP:0.5]{
        \includegraphics[width=0.25\textwidth]{img/map.PNG}
        }
        \caption{模型指标变化}
    \end{figure}
    
    \subitem 可以看出模型效果随着训练逐渐变好。    

\end{enumerate}

\subsection{结果分析}

输入的测试图片和识别图片如下：
 
    \begin{figure}[H]
            \centering
            \subfigure[测试图片]{
            \includegraphics[width=0.4\textwidth]{img/test.PNG}
            }
            \subfigure[识别图片]{
            \includegraphics[width=0.4\textwidth]{img/test_res.png}
            }
            \caption{模型预测图片}
            
    \end{figure}

可以看到白棋和黑棋被识别了出来。

\section{博弈搜索}
\subsection{问题描述}
采用一种博弈搜索算法，实现五子棋博弈程序，其中对棋局状态的判断采用人为设
定函数方式。
\subsection{方法描述}

通过自定义的评估函数返回棋局评分，使用alpha-beta搜索，结合一定的
优化方法，计算得到ai下一步所走的位置。

\subsection{原理描述}

AB剪枝就是minmax搜索的优化。在MAX层，假设当前层已经搜索到一个最大值 X， 
如果发现下一个节点的下一层（也就是MIN层）
会产生一个比X还小的值，那么就直接剪掉此节点。在MIN层，
假设当前层已经搜索到一个最小值 Y， 如果发现下一个节点的下一层（也就是MAX层）
会产生一个比Y还大的值，那么就直接剪掉此节点。

当搜索到达了最大层数，对节点进行评分，根据不同节点评分进行剪枝。
搜索完毕后，走分数最大的位置。

\subsection{步骤描述}

\begin{enumerate}
    \item 编写自定义的评估函数
    
    \subitem 评估函数采用了常用的根据形成的棋型打分的方式
    对于形成的不同棋型，如活四、活三、死四等，给予不同的分数。
    对于敌我双方分数的判定，没有进行特殊的分析方式，而是分别
    计算双方的总分，然后敌方分数降低一半进行相减，得到局面分数。

    \subitem 评估函数的实现方式在C++/code\_file/Evaluation.cpp文件中。
    \item 编写获取搜索位置的函数
    
    \subitem 根据五子棋“战场”的局部性，我认为下载偏远地方的
    棋子对于胜率影响不大，所以对于一个位置，只有它周围有棋子
    的时候，才进行搜索。

    \subitem 可以根据启发式函数获取搜索位置，但我试着实现计算
    空位得分方式进行启发式搜索，发现结果反而更慢。。。于是就
    放弃了启发式搜索。

    \subitem 搜索位置函数实现方式在C++/code\_file/Get\_position.cpp文件中。
    \item 编写搜索函数
    
    \subitem 首先调用获取搜索位置函数获得搜索位置，然后
    进行alpha-beta搜索，当搜索到达最大层数时，调用评估函数
    ，返回评估值。

    \subitem 在搜索的过程中，需要判断局面是否结束，
    如果敌方胜利，则上一步的落子是失败的，不能下在这。

    \subitem 搜索函数实现方式C++/code\_file/AB\_search.cpp文件中。

\end{enumerate}

\subsection{结果分析}

设置搜索层数为4时，ai搜索一些步骤耗费时间较长，可能是评估
函数定义得不够好，导致alpha-beta发生的剪枝次数较少，ai搜索
轻情况太多，搜索时间较长。

设置搜索层数为2时，ai能够较快地得出结果，并且具有一定的
棋力。

对于搜索效果的进一步优化，我认为从增加剪枝方法，启发式方法
进行优化，搜索过程中存在搜索重复局面的情况，所以可以通过一种
算法保存局面和评分，如果在次搜索到同一局面，可以直接获得分数，
不用继续搜索。启发式方法可能的做法为检测双活三和冲四等等，
缩小搜索范围。


\section{进化计算}
\subsection{问题描述}
将上述博弈搜索算法中判断棋局状态的函数改为一种人工神经网络模型，并采用进
化计算方法对该人工神经网络模型来进行学习，使得五子棋博弈程序的下棋水平不断提高。
\subsection{方法描述}

在进行方法说明前，我得吐槽一句，从一开始我就对遗传算法优化
神经网络的效果是不报任何希望的。不说neat算法同时进化网络
结构和权值，所耗的时间无法估计。找了一个利用遗传算法优化网络权值的论文，
一看，方法是首先优化得到粗略的权值，最后还是用梯度下降训练
权值，好家伙，再一看，才几百个参数。对于225的输入数量，上万
的权值数，这点参数还不够塞牙缝的。

我将这个问题视为一个多分类问题，输入当前局面，输出为各点的落子
概率。以网上找的棋局作为训练数据，进行监督学习。

首先确定网络结构，输入层节点数为225，一个隐含层，节点数为300，
输出层，节点数为225，最后输出值进行softmax归一化，预测下一步
的落子。适应度函数为交叉熵的倒数，标签为棋手下一步的落子位置。
根据各网络的适应度进行选择操作，之后进行交叉，变异操作，不断进行
迭代。

\subsection{原理描述}

开始时本来不准备用监督学习的，打算以第二位的搜索ai作为环境，
坚持的时间越久，适应度越高。但是每个网络都在12步之内就输掉，
适应度也一直不会升高。想想还是不太靠谱，就换成监督学习了。

多分类的监督学习任务常用的损失函数为交叉熵函数，用遗传算法
训练网络权值，让交叉熵函数最小，所以以交叉熵函数的倒数作为
适应度函数。

\subsection{步骤描述}

实现代码位于/Python/Evaluation/Net.py文件中。

\begin{enumerate}
    \item 适应度函数
    
    \subitem 输入棋局中某一时刻的棋面，通过网络得到下到每个点的概率，
    以棋手下一步的落子位置作为标签，计算交叉熵，因为要求
    交叉熵越小越好，所以适应度为交叉熵的倒数。

    \item 选择操作
    
    \subitem 计算10个网络的适应度，根据轮盘赌选择保留的种群。

    \item 交叉操作
    
    \subitem 对于每一个种群，生成一个随机数，如果小于0.88，则加入
    队列进行交叉操作。

    \subitem 对于一次交叉操作，随机生成两个数a,b，替换两个
    网络[a,b)位置上的权值。

    \item 变异操作
    
    \subitem 对于网络中的每一个参数，生成一个随机数，如果小于
    0.01，进行变异操作。

    \subitem 对于一次变异操作，可能变异的值为$x\times10^y,x\in [0,1),y\in [0,1)$。

\end{enumerate}

\subsection{结果分析}

下面是迭代了100代后平均适应度和最好交叉熵的变化：

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{img/e.png}
    \caption{平均适应度和最好交叉熵}
\end{figure}

可能是我没写对吧。。。种群的适应度没有什么太大的变化，可能
是交叉操作过于简单，对于网络生成的权值影响较小。

对于遗传算法这块，没啥好说的，可能我运气不好，不适合这种运气算法吧。
我感觉，这种遗传算法只能起辅助作用，作为训练的核心还是得好好考虑。

\section{强化学习}
\subsection{问题描述}
采用强化学习算法对上述人工神经网络模型进行学习，使得五子棋博弈程序的下棋
水平不断提高。
\subsection{方法描述}

在DQN中采用的网络结构为三层卷积层，三层全连接层，输入为通道为4的15x15
的棋盘，4个通道按照落子顺序分别保存我方两步落子和敌方两步落子，如下：
\nopagebreak

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{img/net.PNG}
    \caption{神经网络结构}
\end{figure}

可用的迭代方式为两种，一种是以当前的网络进行落子，直到下完一整局，
随机抽取其中部分局面进行训练。还有一种就是每一步训练一次网络。在实际
操作过程中，发现前一种loss无法收敛，猜测是因为每隔一盘训练一次，训练数据
变化过大，导致网络无法有效地拟合。而后一种loss呈现出收敛的趋势，
所以网络的训练方式采用后者。

对于网络的训练，采用了和随机落子，搜索落子，相同网络落子对抗的三种方式生成
训练数据。最后虽然没有达到想要的结果，但ai的智能还是呈现上升的趋势。

\subsection{原理描述}

采用Q-learning算法，对于常规的Q-learning算法，核心公式为：

$$Q(S,A)\leftarrow (1-\alpha)Q(S,A)+\alpha[R(S,A)+\gamma \max\limits_{a}Q(S^{'},a)]$$

通过不断迭代使Q值接近每一步的真实收益。当迭代稳定时，每一步选取
最大Q值即可获得最优解。

Q-learning算法中，关键就是Q值的计算，在计算Q值时，往往需要一张
Q值表保存每一状态的所有动作对应的Q值。然而五子棋棋盘的状态数为$3^{225}\approx 1e107$。
显然无法保存五子棋所有的状态。这时候需要神经网络强大的拟合能力保存
棋盘的状态。

对于神经网络的损失函数，定义如下：

$$L_i(\theta_i)=[r+\gamma\max\limits_{a^{'}}Q(s^{'},a^i;\theta^{-}_i)-Q(s,a;\theta_i)]^2$$

通过计算网络的输出值与目标网络输出值的MSE计算loss，然后进行反向传播，
根据梯度更新权值。

\subsection{步骤描述}

关于DQN的实现代码位于Python/DQN/ANN.py文件中。

\begin{enumerate}
    \item 双网络
    
    \subitem 为了让网络能够收敛，使用双网络，一个为当前的落子网络，
    一个为目标网络，每隔40步，目标网络更新一次权值，将落子网络的
    权值复制到目标网络中。

    \item 经验回放
    
    \subitem 在网络训练的过程中，因为每一步落子都是连续有关联的，而训练
    数据要求独立分布，不然网络可能会记住这种关系导致效果不好，
    所以需要经验回放。
    
    \subitem 将所下的步骤存入记忆池中，每一步训练时，
    从记忆池中随机抽取一个batch大小的数据进行训练，打破了数据间
    的关联性。

    \item 奖励函数
    
    \subitem 首先说明，我并没有解决DQN过程中奖励稀疏的问题，还是
    采用开始的方法，获胜得5分，失败得-5分，其余得0分的奖励方式。

    \subitem 这是在网络训练中最头疼的问题，alpha系列都使用了
    蒙特卡洛树搜索作为奖励，很遗憾我没有写蒙特卡洛树搜索。

    \subitem 因为一开始我是直接将搜索ai作为训练对手，我的想法是，
    对于一个返回确定结果的对手，这样整个游戏就变成了一个确定的搜索
    树，只要在树的叶子节点也就是输赢节点标记奖励，路径上的奖励置为0就行，
    ai就可以训练得到确定的Q值。

    \subitem 事实证明我的想法还是太过天真，DQN训练过程中存在
    “稀疏奖励”问题。稀疏奖励问题是指agent探索的过程中难以获得正奖励，
    导致学习缓慢甚至无法进行学习的问题。如何解决奖励稀疏问题呢？
    一种方式是类似于人工生成评估函数，类似于ai形成活三，活四给予一定
    奖励，但这种方式太过生硬，实际上ai并没有自己学会这个博弈的诀窍。

    \subitem 为了不太过介入ai的学习过程，我的解决方案为给与每一个
    位置评分，越靠近中心分数越高。但在实践过程中我发现，除了收敛
    速度有所提升，结果没有很大的变化，面对随机落子，还是按照固定
    的走法获胜，面对搜索ai，还是随机落子。所以我认为这是一个失败的
    改动，所以采取了开始的奖励函数。
    
    \item 对于网络结构的处理
    
    \subitem 在开始的网络训练的时候，我直接输入的是当前棋面，
    结果网络的收敛速度非常慢，在查阅资料发现,aplha-go的实现方式
    是保存我方8步和敌方8步的落子信息，可能这种连续的落子信息能够
    让网络找到有效的信息。修改之后网络的收敛速度确实有所提升。
    
\end{enumerate}

\subsection{结果分析}

首先看一下整局训练和每一步训练的loss变化：
\nopagebreak

\begin{figure}[H]
    \centering
    \subfigure[整局训练]{
        \includegraphics[width=0.4\textwidth]{img/rand-a.png}
    }
    \subfigure[每步训练]{
        \includegraphics[width=0.4\textwidth]{img/rand.png}
    }
    \caption{训练方式不同的loss变化}
\end{figure}

可以看出整局训练的loss并没有收敛，而每步训练的方式loss呈现收敛
的趋势。从实际测试中，我也看出，整局训练的落子还是杂乱无章，而
每步训练已经会按照固定的取胜步骤进行落子。


在对战随机落子、搜索落子、自我训练的ai中，loss变化分别如下：
\nopagebreak

\begin{figure}[H]
    \centering
    \subfigure[随机落子]{
        \includegraphics[width=0.4\textwidth]{img/rand.png}
    }
    \subfigure[搜索落子]{
        \includegraphics[width=0.4\textwidth]{img/ai.png}
    }
    \subfigure[自我训练]{
        \includegraphics[width=0.4\textwidth]{img/ANN.png}
    }
    \caption{对战不同ai的loss变化}
\end{figure}

可以看出网络是逐渐收敛的。

上面已经说明了对于随机落子的ai，网络训练得到的效果是比较好的。

而对于搜索落子的情况，虽然网络呈现收敛的趋势，但是实际效果并不是
和搜索算法打得“有来有回”，而是和未训练时一样几步就输掉了。

在看了网络输出的每一步的Q值，我发现网络对于同一状态的每个动作的Q值大致相同，
出现了类似反正怎么下都赢不了，随机下出一步算了的情况。稀疏奖励和一直
找不到可行解的问题使得ai发现只要对每一个动作输出相同的权值，除了在结束时结果
不符合以外，其他情况输出值和目标值大致相同，这就导致了出现了loss
收敛，实际效果却不好的情况。

对于自我训练的ai，loss呈现收敛的趋势，而且相较于一开始双方随机
乱下，训练后的ai落子更加集中，而且更加靠近中心位置，不会出现下在
角落或者边缘的惊人举动。虽然对于依然存在“四子不连”的举动，但还是
可以看出ai的智能有一定的提升。

对于奖励函数难以确定的情况，我想蒙特卡洛树搜索是更好的答案，它能让
学习到的知识更加有效，而不会出现“奖励稀疏”的情况，一个好的奖励
函数能让网络更加有效，更快地收敛。

\end{document}